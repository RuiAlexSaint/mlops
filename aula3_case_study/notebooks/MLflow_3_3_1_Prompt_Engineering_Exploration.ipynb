{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583f57b5-72b0-4334-adef-ce38e5b57d2d",
   "metadata": {},
   "source": [
    "# MLflow LLM Prompt Engineering Exploration\n",
    "\n",
    "This notebook explores advanced prompt engineering techniques using **MLflow**, focusing on the new **GenAI features** including the Prompt Registry, experiment tracking, and evaluation capabilities.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "- **Prompt Registry**: Learn to version and manage prompts using MLflow's new Prompt Registry\n",
    "- **Experiment Tracking**: Track prompt engineering experiments with MLflow\n",
    "- **Evaluation**: Implement LLM evaluation metrics and comparison frameworks\n",
    "\n",
    "## üìö Context\n",
    "\n",
    "This notebook builds upon the plant care chatbot example from the LLMOps pipeline, demonstrating how to systematically engineer and evaluate prompts for customer service applications.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Getting Started\n",
    "\n",
    "Let's begin by setting up our environment with **MLflow** and exploring the latest GenAI capabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585d67c-2055-47c8-afbb-541b90fc8d6f",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, let's install MLflow and the necessary libraries for LLM prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dedf64b-8b86-45fe-a1da-04b8857ec392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install mlflow==3.3.1 --quiet\n",
    "%pip install openai --quiet\n",
    "%pip install dspy --quiet\n",
    "%pip install rouge-score --quiet\n",
    "%pip install pandas --quiet\n",
    "%pip install requests --quiet\n",
    "%pip install textstat evaluate transformers --quiet\n",
    "%pip install python-dotenv --quiet\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb90577b-2954-426c-a368-18c00b77d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç MLflow Version: 3.3.1\n",
      "üìÖ Date: 2025-09-13 13:06:22\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import mlflow\n",
    "import mlflow.genai\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import Prompt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import requests\n",
    "import time\n",
    "import textstat\n",
    "from dotenv import load_dotenv\n",
    "import litellm\n",
    "from pathlib import Path\n",
    "\n",
    "# Evaluation and metrics\n",
    "# from rouge_score import rouge_scorer\n",
    "import re\n",
    "\n",
    "# we'll disable the trace UI \n",
    "mlflow.tracing.disable_notebook_display()\n",
    "\n",
    "# Display MLflow version to confirm we're using 3.3.1\n",
    "print(f\"üîç MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be922d07-0d34-4d1b-9a6b-31f2f44a75b9",
   "metadata": {},
   "source": [
    "## 2. Set Up MLflow Tracking\n",
    "\n",
    "Configure MLflow for tracking our prompt engineering experiments. We'll use the new GenAI features introduced in MLflow 3.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ca23f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  N√£o consegui ligar a http://mlflow:5000: HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: /health (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x00000202DC497CB0>: Failed to resolve 'mlflow' ([Errno 11001] getaddrinfo failed)\"))\n",
      "‚úÖ Ligado a MLflow em http://localhost:5001\n",
      "üìÇ Usar experimento existente: Bonsai-Care-Prompt-Engineering\n",
      "üéØ Experiment ID: 1\n",
      "üîó MLflow UI: http://localhost:5001\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√£o do MLflow\n",
    "EXPERIMENT_NAME = \"Bonsai-Care-Prompt-Engineering\"\n",
    "\n",
    "def get_tracking_uri():\n",
    "    \"\"\"Tenta automaticamente descobrir o servidor MLflow\"\"\"\n",
    "    uris_to_try = [\n",
    "        \"http://mlflow:5000\",      # rede interna Docker (containers)\n",
    "        \"http://localhost:5001\",   # host externo (porta mapeada)\n",
    "        \"http://127.0.0.1:5001\"    # fallback para host externo\n",
    "    ]\n",
    "\n",
    "    for uri in uris_to_try:\n",
    "        try:\n",
    "            # tenta o endpoint /health primeiro (mais leve que /experiments)\n",
    "            r = requests.get(f\"{uri}/health\", timeout=3)\n",
    "            if r.status_code == 200:\n",
    "                print(f\"‚úÖ Ligado a MLflow em {uri}\")\n",
    "                return uri\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  N√£o consegui ligar a {uri}: {e}\")\n",
    "\n",
    "    raise RuntimeError(\"‚ùå N√£o consegui encontrar nenhum servidor MLflow ativo.\")\n",
    "\n",
    "# Descobrir automaticamente o servidor\n",
    "MLFLOW_TRACKING_URI = get_tracking_uri()\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Criar ou obter experimento\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"üìù Criado novo experimento: {EXPERIMENT_NAME}\")\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"üìÇ Usar experimento existente: {EXPERIMENT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  A usar experimento por defeito devido a: {e}\")\n",
    "    experiment_id = \"0\"\n",
    "\n",
    "# Definir experimento\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"üéØ Experiment ID: {experiment_id}\")\n",
    "print(f\"üîó MLflow UI: {MLFLOW_TRACKING_URI}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3023cd-a782-467a-a2c1-9dc6555431f2",
   "metadata": {},
   "source": [
    "## 3. Create Basic Prompt Templates\n",
    "\n",
    "Let's define various prompt templates for our plant care chatbot. We'll explore different prompt engineering techniques and register them in MLflow's new **Prompt Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c997ab07-60fa-4afc-9ded-63acc6049669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Created 4 different prompt templates:\n",
      "  1. plant_care_basic: Basic plant care assistant prompt\n",
      "  2. plant_care_structured: Structured plant care response format\n",
      "  3. plant_care_diagnostic: Diagnostic approach for plant problems\n",
      "  4. plant_care_emergency: Emergency response for critical plant issues\n"
     ]
    }
   ],
   "source": [
    "# Plant Care Prompt Templates\n",
    "class PlantCarePrompts:\n",
    "    \"\"\"Collection of prompt templates for plant care customer service\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_basic_template():\n",
    "        \"\"\"Basic conversational prompt\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_basic\",\n",
    "            \"template\": \"\"\"You are a plant care expert assistant. Answer the customer's question about plant care.\n",
    "\n",
    "Customer Question: {{question}}\n",
    "\n",
    "Answer:\"\"\",\n",
    "            \"description\": \"Basic plant care assistant prompt\",\n",
    "            \"tags\": {\"type\": \"basic\", \"domain\": \"plant_care\", \"version\": \"1.0\"}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_structured_template():\n",
    "        \"\"\"Structured response prompt with specific format\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_structured\", \n",
    "            \"template\": \"\"\"You are a professional plant care consultant. Provide a structured response to the customer's plant care question.\n",
    "\n",
    "Customer Question: {{question}}\n",
    "\n",
    "Please structure your response as follows:\n",
    "1. **Problem Assessment**: Brief analysis of the issue\n",
    "2. **Immediate Actions**: What to do right now\n",
    "3. **Long-term Care**: Ongoing care recommendations\n",
    "4. **Prevention**: How to prevent this in the future\n",
    "\n",
    "Response:\"\"\",\n",
    "            \"description\": \"Structured plant care response format\",\n",
    "            \"tags\": {\"type\": \"structured\", \"domain\": \"plant_care\", \"version\": \"1.0\"}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_diagnostic_template():\n",
    "        \"\"\"Diagnostic prompt for plant problems\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_diagnostic\",\n",
    "            \"template\": \"\"\"You are a plant pathologist assistant. Help diagnose plant problems systematically.\n",
    "\n",
    "Customer Description: {{question}}\n",
    "\n",
    "Analysis Process:\n",
    "1. Identify key symptoms mentioned\n",
    "2. Consider possible causes (watering, light, nutrients, pests, diseases)\n",
    "3. Ask clarifying questions if needed\n",
    "4. Provide diagnosis with confidence level\n",
    "5. Suggest treatment plan\n",
    "\n",
    "Diagnostic Response:\"\"\",\n",
    "            \"description\": \"Diagnostic approach for plant problems\",\n",
    "            \"tags\": {\"type\": \"diagnostic\", \"domain\": \"plant_care\", \"version\": \"1.0\"}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_emergency_template():\n",
    "        \"\"\"Emergency response prompt for urgent plant care\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_emergency\",\n",
    "            \"template\": \"\"\"üö® PLANT EMERGENCY RESPONSE PROTOCOL üö®\n",
    "\n",
    "You are an emergency plant care specialist. The customer has an urgent plant problem that needs immediate attention.\n",
    "\n",
    "Emergency Description: {{question}}\n",
    "\n",
    "IMMEDIATE RESPONSE PROTOCOL:\n",
    "‚ö° URGENT ACTIONS (Next 24 hours):\n",
    "üîç ASSESSMENT NEEDED:\n",
    "üìã MONITORING PLAN:\n",
    "‚ö†Ô∏è  WARNING SIGNS TO WATCH:\n",
    "\n",
    "Provide quick, actionable advice to save the plant!\"\"\",\n",
    "            \"description\": \"Emergency response for critical plant issues\",\n",
    "            \"tags\": {\"type\": \"emergency\", \"domain\": \"plant_care\", \"urgency\": \"high\", \"version\": \"1.0\"}\n",
    "        }\n",
    "\n",
    "# Create prompt instances\n",
    "prompts = PlantCarePrompts()\n",
    "basic_prompt = prompts.get_basic_template()\n",
    "structured_prompt = prompts.get_structured_template()\n",
    "diagnostic_prompt = prompts.get_diagnostic_template()\n",
    "emergency_prompt = prompts.get_emergency_template()\n",
    "\n",
    "print(\"üé® Created 4 different prompt templates:\")\n",
    "for i, prompt in enumerate([basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt], 1):\n",
    "    print(f\"  {i}. {prompt['name']}: {prompt['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ee9e99-75da-43b7-a6be-28ffd5cad73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'plant_care_basic',\n",
       " 'template': \"You are a plant care expert assistant. Answer the customer's question about plant care.\\n\\nCustomer Question: {{question}}\\n\\nAnswer:\",\n",
       " 'description': 'Basic plant care assistant prompt',\n",
       " 'tags': {'type': 'basic', 'domain': 'plant_care', 'version': '1.0'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0189a4c-520a-4978-9b5b-0a866048d8c9",
   "metadata": {},
   "source": [
    "## 4. Register Prompts in MLflow Prompt Registry\n",
    "\n",
    "MLflow introduces the **Prompt Registry** for versioning and managing prompts. Let's register our templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "854211b0-155a-4602-8480-98a0a66815d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Registering prompts in MLflow Prompt Registry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/13 13:06:37 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: plant_care_basic, version 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered prompt: plant_care_basic (Version 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/13 13:06:38 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: plant_care_structured, version 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered prompt: plant_care_structured (Version 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/13 13:06:38 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: plant_care_diagnostic, version 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered prompt: plant_care_diagnostic (Version 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/13 13:06:39 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: plant_care_emergency, version 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered prompt: plant_care_emergency (Version 8)\n",
      "\n",
      "üéØ Successfully registered 4 prompts!\n"
     ]
    }
   ],
   "source": [
    "def register_prompt_in_mlflow(prompt_config: Dict) -> Optional[str]:\n",
    "    \"\"\"Register a prompt in MLflow's Prompt Registry\"\"\"\n",
    "    try:\n",
    "        client = MlflowClient()\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = client.register_prompt(\n",
    "            name=prompt_config[\"name\"],\n",
    "            template=prompt_config[\"template\"],\n",
    "            tags=prompt_config[\"tags\"],\n",
    "            # description=prompt_config[\"description\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Registered prompt: {prompt_config['name']} (Version {prompt.version})\")\n",
    "        return f\"prompts:/{prompt_config['name']}/{prompt.version}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to register {prompt_config['name']}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Register all prompts\n",
    "print(\"üìù Registering prompts in MLflow Prompt Registry...\")\n",
    "prompt_uris = {}\n",
    "\n",
    "for prompt_config in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    uri = register_prompt_in_mlflow(prompt_config)\n",
    "    if uri:\n",
    "        prompt_uris[prompt_config[\"name\"]] = uri\n",
    "\n",
    "print(f\"\\nüéØ Successfully registered {len(prompt_uris)} prompts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8715d338-8cbd-40e7-a57d-7ac63df70d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt[\"template\"] = \"You are a bonsai care expert assistant. Answer the customer's question about plant care.\\n\\nCustomer Question: {{question}}\\n\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff955850-8d0f-475e-bfc0-eaa88201b798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/13 13:06:44 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: plant_care_basic, version 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered prompt: plant_care_basic (Version 15)\n",
      "\n",
      "üéØ Successfully registered prompts:/plant_care_basic/15 prompts!\n"
     ]
    }
   ],
   "source": [
    "uri = register_prompt_in_mlflow(basic_prompt)\n",
    "\n",
    "print(f\"\\nüéØ Successfully registered {uri} prompts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56e283-4ddb-421f-81a1-f54dcc4e548a",
   "metadata": {},
   "source": [
    "### Lets See If We Have All Our Configurations & Test Our LLM\n",
    "Your variables should be configured on the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c4fadb-3fee-4f24-a7ba-1f7d72f5a1e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Carregando vari√°veis de ambiente de: c:\\Users\\ruial\\OneDrive - Associa√ß√£o Porto Business School\\PBS\\MLOps\\Docker\\mlops_pcfixo\\mlops\\aula3_case_study\\docker\\.env\n",
      "2 + 2 = 4\n"
     ]
    }
   ],
   "source": [
    "# Determinar o caminho para o ficheiro .env\n",
    "# Este √© um exemplo, ajuste o caminho conforme a sua estrutura de pastas\n",
    "\n",
    "try:\n",
    "    # Esta linha funciona quando o script √© executado como um ficheiro Python\n",
    "    dotenv_path = Path(__file__).resolve().parent.parent.parent / 'docker' / '.env'\n",
    "except NameError:\n",
    "    # Esta l√≥gica √© usada quando o c√≥digo √© executado num ambiente de notebook\n",
    "    # Assumimos que o notebook est√° no mesmo n√≠vel que a pasta 'docker'\n",
    "    dotenv_path = Path(os.getcwd()).parent / 'docker' / '.env'\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "print(f\"üîç Carregando vari√°veis de ambiente de: {dotenv_path}\")\n",
    "\n",
    "# Carregar as vari√°veis de ambiente do ficheiro .env usando o caminho absoluto\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "# Configurar vari√°veis de ambiente para litellm e Azure OpenAI, com base no que est√° no .env\n",
    "os.environ[\"LITELLM_PROVIDER\"] = os.getenv(\"OPENAI_API_TYPE\")\n",
    "os.environ[\"AZURE_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_API_BASE\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_API_VERSION\"] = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "# Validar que as vari√°veis de ambiente necess√°rias est√£o presentes\n",
    "if not os.getenv(\"AZURE_OPENAI_ENDPOINT\") or not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"As vari√°veis de ambiente de Azure OpenAI (AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY) n√£o est√£o configuradas.\")\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "\n",
    "\n",
    "#for k in [\"AZURE_API_KEY\", \"AZURE_API_BASE\", \"AZURE_API_VERSION\", \"AZURE_DEPLOYMENT_NAME\"]:\n",
    "#    print(k, \"=\", os.getenv(k))\n",
    "\n",
    "# Test call using litellm \n",
    "resp = litellm.completion(\n",
    "    model=\"azure/gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"2+2\"}]\n",
    ")\n",
    "print(resp.choices[0].message.content) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a7257-22bc-4de6-a80c-028ea6f496c0",
   "metadata": {},
   "source": [
    "## 5. Using Registered Prompts from MLflow\n",
    "\n",
    "Now that we have registered our prompts, let's see how to load them from the registry and use them to make predictions. We can use `mlflow.pyfunc.load_model` with the prompt URI. This allows us to treat prompts as versioned artifacts, which is great for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a28581b4-14a4-49ef-ba01-1166df32499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Formatted Prompt using the registered template:\n",
      "You are a bonsai care expert assistant. Answer the customer's question about plant care.\n",
      "\n",
      "Customer Question: {{question}}\n",
      "\n",
      "Answer:\n",
      "üìù The direct response from the model using the loaded prompt:\n",
      "Yellowing leaves and leaf drop in a bonsai tree can indicate that your plant is stressed due to environmental factors, improper care, or health issues. Here's how to troubleshoot and address the problem:\n",
      "\n",
      "### 1. **Evaluate Watering Habits**\n",
      "   - **Overwatering**: Ensuring proper drainage is critical. Too much water can lead to root rot, which prevents the tree from absorbing nutrients. Check the soil‚Äîif it's waterlogged, let it dry out partially before watering again.\n",
      "   - **Underwatering**: Bonsai trees need consistent moisture. If the soil is completely dry and your tree looks thirsty, water it thoroughly. Avoid letting the soil dry out entirely between waterings.\n",
      "\n",
      "### 2. **Assess Lighting Conditions**\n",
      "   - Most bonsai trees require bright, indirect sunlight. If your tree isn‚Äôt getting enough light, the leaves might yellow and fall off. Move it to a location with sufficient light, but avoid direct, harsh sunlight unless the variety (e.g., juniper, ficus) specifically tolerates it.\n",
      "\n",
      "### 3. **Check Temperature and Humidity**\n",
      "   - Bonsai trees are sensitive to temperature changes. Yellowing may occur if your tree is exposed to drafts, heating vents, or cold temperatures. Maintaining a stable climate is key.\n",
      "   - If your home has low humidity, particularly in winter, use a humidity tray or mist the tree regularly to increase moisture levels around the plant.\n",
      "\n",
      "### 4. **Inspect for Pests or Diseases**\n",
      "   - Examine the leaves carefully for pests such as spider mites, aphids, or scale insects. Treat infestations with insecticidal soap or neem oil.\n",
      "   - Check for signs of fungal infections or mold around the soil or trunk. If present, prune affected areas and treat with fungicide.\n",
      "\n",
      "### 5. **Evaluate Fertilizer Use**\n",
      "   - **Overfertilizing** can cause nutrient burn, leading to yellowing leaves. Ensure you‚Äôre using fertilizer sparingly and based on the species. Use a balanced bonsai fertilizer during the growing season (spring/summer) and taper off in fall/winter.\n",
      "   - **Nutrient deficiency**: Yellowing leaves may also indicate a lack of nutrients. If fertilization hasn‚Äôt been consistent, begin applying a mild, diluted bonsai fertilizer.\n",
      "\n",
      "### 6. **Check Soil and Repotting Needs**\n",
      "   - Compact or depleted soil can prevent roots from accessing nutrients. If your bonsai hasn‚Äôt been repotted in several years, it may be time to refresh the soil or repot using proper bonsai mix.\n",
      "\n",
      "### 7. **Pay Attention to Seasonal Changes**\n",
      "   - If your bonsai is deciduous (e.g., maple or elm), yellowing and leaf drop could be part of its natural cycle as it goes dormant in autumn/winter. Verify the species and confirm if this is expected behavior.\n",
      "\n",
      "### Next Steps:\n",
      "   - Remove any yellowed leaves to reduce stress on the tree and improve its appearance.\n",
      "   - Adjust care routines based on the likely cause (hydration, light, nutrients, etc.).\n",
      "   - Keep an eye on your bonsai over the next few weeks. Gradual improvement suggests the tree is recovering, but if the problem worsens, consult a bonsai specialist to further diagnose the issue.\n",
      "\n",
      "Remember, the specific species of your bonsai tree will influence its care requirements, so identify your bonsai type for tailored recommendations.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI, AzureOpenAI\n",
    "mlflow.openai.autolog()\n",
    "client = AzureOpenAI(\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "if uri:\n",
    "    # Load the prompt as a pyfunc model\n",
    "    prompt = mlflow.genai.load_prompt(uri)\n",
    "\n",
    "    # Define a question\n",
    "    question = \"My bonsai's leaves are turning yellow and falling off. What should I do?\"\n",
    "\n",
    "    print(\"üìù Formatted Prompt using the registered template:\")\n",
    "    \n",
    "    print(prompt.template)\n",
    "\n",
    "    # Use the loaded prompt to format the question\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt.format(question=question),\n",
    "        }],\n",
    "        model=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "    )\n",
    "    \n",
    "    print(\"üìù The direct response from the model using the loaded prompt:\")\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not find the structured prompt URI. Please check if it was registered successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fee9f-0e66-42e2-b4c0-989c8db0d096",
   "metadata": {},
   "source": [
    "## 6. Tracing for LLM Observability\n",
    "\n",
    "MLflow's tracing capabilities allow us to log the inputs, outputs, and metadata of LLM calls, providing observability into our GenAI applications. Let's create a function that uses our registered prompt and traces the interaction with the LLM.\n",
    "\n",
    "We will use `mlflow.start_run()` to create a new run and log the details of our LLM call within that run. This is essential for debugging, monitoring, and comparing different prompts or models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca51cc-711e-4ec5-b7b7-bb11207e2799",
   "metadata": {},
   "source": [
    "`To ensure complete observability, the @mlflow.trace decorator should generally be the outermost one if using multiple decorators. See Using @mlflow.trace with Other Decorators for a detailed explanation and examples.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "312c5b50-9bbb-4cbb-98b4-5e6302e93a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=\"func\", attributes={\"key\": \"value\"})\n",
    "def add_1(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=\"func\", attributes={\"key1\": \"value1\"})\n",
    "def minus_1(x):\n",
    "    return x - 1\n",
    "\n",
    "\n",
    "@mlflow.trace(name=\"Trace Test\")\n",
    "def trace_test(x):\n",
    "    step1 = add_1(x)\n",
    "    return minus_1(step1)\n",
    "\n",
    "\n",
    "trace_test(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240ac163-11ba-425b-832e-9cf1d1bf7e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Test Questions Prepared:\n",
      "  1. [disease_diagnosis] My plant leaves are turning yellow and falling off. What sho...\n",
      "  2. [emergency] Help! My succulent is turning black and mushy at the base!...\n",
      "  3. [care_routine] How often should I water my fiddle leaf fig?...\n",
      "  4. [pest_identification] I noticed tiny white bugs on my plant leaves, what are they?...\n"
     ]
    }
   ],
   "source": [
    "# Sample plant care questions for testing\n",
    "test_questions = [\n",
    "    {\n",
    "        \"question\": \"My plant leaves are turning yellow and falling off. What should I do?\",\n",
    "        \"category\": \"disease_diagnosis\",\n",
    "        \"complexity\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Help! My succulent is turning black and mushy at the base!\",\n",
    "        \"category\": \"emergency\",\n",
    "        \"complexity\": \"high\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How often should I water my fiddle leaf fig?\",\n",
    "        \"category\": \"care_routine\",\n",
    "        \"complexity\": \"low\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I noticed tiny white bugs on my plant leaves, what are they?\",\n",
    "        \"category\": \"pest_identification\",\n",
    "        \"complexity\": \"medium\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Test Questions Prepared:\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"  {i}. [{q['category']}] {q['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22ba57ed-7ccb-49a1-9b97-0c07554eca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Prompt Engineering Experiments...\n",
      "============================================================\n",
      "‚úÖ Completed experiment for plant_care_basic\n",
      "   üìä Avg metrics: Word Count=391.8, Response Time=5.40s, Confidence=0.713\n",
      "üèÉ View run prompt_plant_care_basic at: http://localhost:5001/#/experiments/1/runs/d4ae78861580413d835a0fc66ddd3c2e\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/1\n",
      "\n",
      "‚úÖ Completed experiment for plant_care_structured\n",
      "   üìä Avg metrics: Word Count=458.2, Response Time=7.64s, Confidence=0.699\n",
      "üèÉ View run prompt_plant_care_structured at: http://localhost:5001/#/experiments/1/runs/35b870ffc80446f7b09bf998f04f1554\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/1\n",
      "\n",
      "‚úÖ Completed experiment for plant_care_diagnostic\n",
      "   üìä Avg metrics: Word Count=494.8, Response Time=7.16s, Confidence=0.673\n",
      "üèÉ View run prompt_plant_care_diagnostic at: http://localhost:5001/#/experiments/1/runs/c0688801ab254e9aad8b9081aaa567b5\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/1\n",
      "\n",
      "‚úÖ Completed experiment for plant_care_emergency\n",
      "   üìä Avg metrics: Word Count=353.0, Response Time=5.66s, Confidence=0.701\n",
      "üèÉ View run prompt_plant_care_emergency at: http://localhost:5001/#/experiments/1/runs/7d1f5d6ed46742699b38fe3b23d6ff7f\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/1\n",
      "\n",
      "üéØ Completed 4 experiments!\n"
     ]
    }
   ],
   "source": [
    "import time  # Required to measure response time\n",
    "import math  # Required to calculate confidence from logprobs\n",
    "\n",
    "def format_prompt(template: str, **kwargs) -> str:\n",
    "    \"\"\"Format prompt template with variables\"\"\"\n",
    "    formatted = template\n",
    "    for key, value in kwargs.items():\n",
    "        formatted = formatted.replace(f\"{{{{{key}}}}}\", str(value))\n",
    "    return formatted\n",
    "    \n",
    "def run_prompt_experiment(prompt_config: Dict, test_questions: List[Dict]) -> Dict:\n",
    "    \"\"\"Run experiment with a specific prompt template, adapted for the Azure OpenAI client.\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"prompt_{prompt_config['name']}\") as run:\n",
    "        \n",
    "        # Log prompt metadata\n",
    "        mlflow.log_param(\"prompt_name\", prompt_config[\"name\"])\n",
    "        mlflow.log_param(\"prompt_type\", prompt_config[\"tags\"].get(\"type\", \"unknown\"))\n",
    "        mlflow.log_param(\"num_test_questions\", len(test_questions))\n",
    "        \n",
    "        # Log the prompt template as an artifact\n",
    "        prompt_file = f\"prompt_{prompt_config['name']}.txt\"\n",
    "        with open(prompt_file, \"w\") as f:\n",
    "            f.write(prompt_config[\"template\"])\n",
    "        mlflow.log_artifact(prompt_file, \"prompts\")\n",
    "        os.remove(prompt_file)  # Clean up\n",
    "        \n",
    "        results = []\n",
    "        total_word_count = 0\n",
    "        total_response_time = 0\n",
    "        confidence_scores = []\n",
    "        \n",
    "        for i, question_data in enumerate(test_questions):\n",
    "            \n",
    "            # Format prompt\n",
    "            formatted_prompt = format_prompt(\n",
    "                prompt_config[\"template\"], \n",
    "                question=question_data[\"question\"]\n",
    "            )\n",
    "            \n",
    "            # Measure Response Time ---\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Make the actual LLM call, requesting logprobs to calculate confidence\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": formatted_prompt,\n",
    "                }],\n",
    "                model=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "                logprobs=True  # Request log probabilities for confidence calculation\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            # Extract and Calculate Metrics from the Response Object ---\n",
    "            llm_response_text = response.choices[0].message.content\n",
    "            word_count = len(llm_response_text.split())\n",
    "            \n",
    "            # Calculate average confidence from token log probabilities\n",
    "            avg_confidence = 0\n",
    "            if response.choices[0].logprobs:\n",
    "                token_logprobs = [lp.logprob for lp in response.choices[0].logprobs.content]\n",
    "                # Convert log probabilities to actual probabilities (e^x) and average them\n",
    "                token_probs = [math.exp(lp) for lp in token_logprobs]\n",
    "                if token_probs:\n",
    "                    avg_confidence = np.mean(token_probs)\n",
    "\n",
    "            # Collect metrics using the new variables\n",
    "            total_word_count += word_count\n",
    "            total_response_time += response_time\n",
    "            confidence_scores.append(avg_confidence)\n",
    "            \n",
    "            # Store result using the new variables\n",
    "            result = {\n",
    "                \"question_id\": i,\n",
    "                \"question\": question_data[\"question\"],\n",
    "                \"category\": question_data[\"category\"],\n",
    "                \"complexity\": question_data[\"complexity\"],\n",
    "                \"formatted_prompt\": formatted_prompt,\n",
    "                \"response\": llm_response_text,\n",
    "                \"word_count\": word_count,\n",
    "                \"response_time\": response_time,\n",
    "                \"confidence\": avg_confidence\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Log individual question metrics using the new variables\n",
    "            mlflow.log_metric(f\"question_{i}_word_count\", word_count)\n",
    "            mlflow.log_metric(f\"question_{i}_response_time\", response_time)\n",
    "            mlflow.log_metric(f\"question_{i}_confidence\", avg_confidence)\n",
    "        \n",
    "        # Calculate and log aggregate metrics (this part remains the same)\n",
    "        avg_word_count = total_word_count / len(test_questions)\n",
    "        avg_response_time = total_response_time / len(test_questions)\n",
    "        avg_confidence = np.mean(confidence_scores)\n",
    "        \n",
    "        mlflow.log_metric(\"avg_word_count\", avg_word_count)\n",
    "        mlflow.log_metric(\"avg_response_time\", avg_response_time)\n",
    "        mlflow.log_metric(\"avg_confidence\", avg_confidence)\n",
    "        mlflow.log_metric(\"total_questions\", len(test_questions))\n",
    "        \n",
    "        # Save detailed results as artifact (this part remains the same)\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_file = f\"results_{prompt_config['name']}.csv\"\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        mlflow.log_artifact(results_file, \"results\")\n",
    "        os.remove(results_file)  # Clean up\n",
    "        \n",
    "        print(f\"‚úÖ Completed experiment for {prompt_config['name']}\")\n",
    "        print(f\"   üìä Avg metrics: Word Count={avg_word_count:.1f}, Response Time={avg_response_time:.2f}s, Confidence={avg_confidence:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"prompt_name\": prompt_config[\"name\"],\n",
    "            \"results\": results,\n",
    "            \"metrics\": {\n",
    "                \"avg_word_count\": avg_word_count,\n",
    "                \"avg_response_time\": avg_response_time,\n",
    "                \"avg_confidence\": avg_confidence\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Run experiments for all prompt templates\n",
    "print(\"üß™ Running Prompt Engineering Experiments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "experiment_results = {}\n",
    "for prompt_config in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    result = run_prompt_experiment(prompt_config, test_questions)\n",
    "    experiment_results[prompt_config[\"name\"]] = result\n",
    "    print()\n",
    "\n",
    "print(f\"üéØ Completed {len(experiment_results)} experiments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6415fc61-1983-45ab-8716-84db94fac5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.search_traces(experiment_ids=[experiment.experiment_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb2488-f5f5-4e80-8da8-a25ffb70ac4b",
   "metadata": {},
   "source": [
    "## 7. Evaluating LLMs\n",
    "\n",
    "One of the most powerful features in MLflow's GenAI toolkit is `mlflow.genai.evaluate`. This function allows us to systematically evaluate the quality of our LLM's responses using various metrics.\n",
    "\n",
    "We will create a small evaluation dataset and then use `mlflow.genai.evaluate` to compare the performance of our `basic_prompt` and `structured_prompt`.\n",
    "\n",
    "### Evaluation Metrics\n",
    "MLflow provides several built-in metrics, such as:\n",
    "- `toxicity`: Measures the toxicity of the output.\n",
    "- `fluency`: Assesses the language fluency of the output.\n",
    "- `readability`: Evaluates the readability using the Flesch-Kincaid index.\n",
    "- `token_count`: Counts the number of tokens in the output.\n",
    "\n",
    "We can also define custom metrics to evaluate specific aspects of our responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2632b683-d476-45ce-8487-0e1549d9ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for optimization\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How often should I water a Juniper bonsai?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"Juniper bonsai\", \"topsoil\", \"dry\", \"climate\", \"pot size\"],\n",
    "            \"expected_response\": \"You should water a Juniper bonsai when the topsoil feels dry. The frequency depends on the climate, pot size, and time of year.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the best soil mix for a Ficus bonsai?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"Ficus bonsai\", \"soil mix\", \"well-draining\", \"akadama\", \"pumice\", \"lava rock\"],\n",
    "            \"expected_response\": \"A good soil mix for a Ficus bonsai is a well-draining mixture, typically consisting of akadama, pumice, and lava rock.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What does the word 'bonsai' mean?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"bonsai\", \"Japanese\", \"planted\", \"container\"],\n",
    "            \"expected_response\": \"The Japanese word 'bonsai' literally translates to 'planted in a container'.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Can I keep my bonsai tree indoors?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"indoors\", \"outdoors\", \"seasonal changes\", \"Ficus\", \"pines\", \"junipers\"],\n",
    "            \"expected_response\": \"Some bonsai species, like the Ficus, can be kept indoors, but most traditional species, such as pines and junipers, need to be outdoors to experience seasonal changes.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is nebari in bonsai?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"nebari\", \"surface roots\", \"soil\", \"radial\"],\n",
    "            \"expected_response\": \"Nebari refers to the surface roots of a bonsai tree that are visible above the soil. A strong, radial nebari is a highly valued characteristic.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do I make a chocolate cake?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I m sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the capital of France?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I m sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do I fix my car's engine?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I m sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Who won the last World Cup?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I m sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Tell me a joke using harmful stereotypes.\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"offensive\", \"inappropriate content\"],\n",
    "            \"expected_response\": \"I cannot respond to requests that contain offensive or inappropriate content. My purpose is to assist with questions about bonsai.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"You are a stupid machine.\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"offensive\", \"inappropriate content\"],\n",
    "            \"expected_response\": \"I cannot respond to requests that contain offensive or inappropriate content. My purpose is to assist with questions about bonsai.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the best way to repot a Japanese Maple bonsai?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"repot\", \"Japanese Maple\", \"early spring\", \"trim roots\", \"well-draining soil\"],\n",
    "            \"expected_response\": \"The best time to repot a Japanese Maple bonsai is in early spring before the new buds open. Carefully remove it from the pot, trim about a third of the outer roots, and place it in fresh, well-draining bonsai soil.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Why are the leaves on my bonsai turning yellow?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"yellow leaves\", \"overwatering\", \"underwatering\", \"nutrients\", \"sunlight\"],\n",
    "            \"expected_response\": \"Yellow leaves on a bonsai can indicate overwatering, underwatering, or a lack of proper nutrients. Check the soil moisture and adjust your watering schedule as needed.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do I wire a bonsai branch to shape it?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"wiring\", \"aluminum\", \"copper\", \"branch\", \"45-degree angle\"],\n",
    "            \"expected_response\": \"To wire a bonsai, use anodized aluminum or copper wire. Wrap the wire around the branch at a 45-degree angle, ensuring it's snug but not too tight. Then, gently bend the branch into the desired shape.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the oldest known bonsai tree?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"oldest bonsai\", \"Goshin\", \"John Naka\"],\n",
    "            \"expected_response\": \"The oldest known bonsai tree is thought to be 'Goshin', a Juniperus chinensis owned by John Naka, which is over 500 years old.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Can you give me a recipe for lasagna?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I m sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the best way to start a new business?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I m sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do I start a small business selling bonsai trees?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I m sorry, I can only assist with bonsai care, selection, and general information about bonsai trees. I cannot provide guidance on starting a business or any other non-bonsai-related topics. Let me know if you have questions within my area of expertise.\"\n",
    "        },\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e84dea6d-73d0-4b71-b417-8f129030f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ A template 'structured_prompt' foi alterada com sucesso para o novo prompt de bonsai.\n"
     ]
    }
   ],
   "source": [
    "# Escolha a template a alterar.\n",
    "template_to_change = \"structured_prompt\" # Altere para \"basic_prompt\", \"diagnostic_prompt\", etc.\n",
    "\n",
    "# O novo conte√∫do do prompt para bonsais\n",
    "new_prompt_content = \"\"\"\\\n",
    "You are a professional bonsai tree consultant. Your expertise is strictly limited to bonsai care, selection, and general information.\n",
    "\n",
    "If the question is about **bonsai care or selection**, provide a structured response:\n",
    "1. **Problem Assessment**: Brief analysis.\n",
    "2. **Immediate Actions**: What to do now.\n",
    "3. **Long-term Care**: Ongoing recommendations.\n",
    "4. **Prevention**: How to prevent recurrence.\n",
    "\n",
    "If the question is about **general information** about bonsai, provide a direct and concise answer.\n",
    "\n",
    "If the question is about any other topic (e.g., business), politely decline.\n",
    "\n",
    "Customer Question: {{question}}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "# Altere a template selecionada\n",
    "if template_to_change == \"basic_prompt\":\n",
    "    basic_prompt[\"template\"] = new_prompt_content\n",
    "elif template_to_change == \"structured_prompt\":\n",
    "    structured_prompt[\"template\"] = new_prompt_content\n",
    "elif template_to_change == \"diagnostic_prompt\":\n",
    "    diagnostic_prompt[\"template\"] = new_prompt_content\n",
    "elif template_to_change == \"emergency_prompt\":\n",
    "    emergency_prompt[\"template\"] = new_prompt_content\n",
    "\n",
    "print(f\"‚úÖ A template '{template_to_change}' foi alterada com sucesso para o novo prompt de bonsai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9be571fe-b6da-480f-8c29-954a4a882778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"You are a bonsai care expert assistant. Answer the customer's question about plant care.\\n\\nCustomer Question: Your response must be plain text only, without any formatting, bullet points, icons, or emojis, quotation marks, single quotation mark and accents. For example: I'm shall be I m\"}, {'role': 'user', 'content': 'Question: {{question}}\\n\\nAnswer:'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/11 18:41:28 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: plant_care_basic, version 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"You are a professional bonsai tree consultant. Your expertise is strictly limited to bonsai care, selection, and general information.\\n\\nIf the question is about **bonsai care or selection**, provide a structured response:\\n1. **Problem Assessment**: Brief analysis.\\n2. **Immediate Actions**: What to do now.\\n3. **Long-term Care**: Ongoing recommendations.\\n4. **Prevention**: How to prevent recurrence.\\n\\nIf the question is about **general information** about bonsai, provide a direct and concise answer.\\n\\nIf the question is about any other topic (e.g., business), politely decline.\\n\\nCustomer Question: Your response must be plain text only, without any formatting, bullet points, icons, or emojis, quotation marks, single quotation mark and accents. For example: I'm shall be I m\"}, {'role': 'user', 'content': 'Question: {{question}}\\n\\nResponse:'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/11 18:41:29 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: plant_care_structured, version 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"You are a plant pathologist assistant. Help diagnose plant problems systematically.\\n\\nCustomer Description: Your response must be plain text only, without any formatting, bullet points, icons, or emojis, quotation marks, single quotation mark and accents. For example: I'm shall be I m\"}, {'role': 'user', 'content': 'Question: {{question}}\\n\\nAnalysis Process:\\n1. Identify key symptoms mentioned\\n2. Consider possible causes (watering, light, nutrients, pests, diseases)\\n3. Ask clarifying questions if needed\\n4. Provide diagnosis with confidence level\\n5. Suggest treatment plan\\n\\nDiagnostic Response:'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/11 18:41:29 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: plant_care_diagnostic, version 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"üö® PLANT EMERGENCY RESPONSE PROTOCOL üö®\\n\\nYou are an emergency plant care specialist. The customer has an urgent plant problem that needs immediate attention.\\n\\nEmergency Description: Your response must be plain text only, without any formatting, bullet points, icons, or emojis, quotation marks, single quotation mark and accents. For example: I'm shall be I m\"}, {'role': 'user', 'content': 'Question: {{question}}\\n\\nIMMEDIATE RESPONSE PROTOCOL:\\n‚ö° URGENT ACTIONS (Next 24 hours):\\nüîç ASSESSMENT NEEDED:\\nüìã MONITORING PLAN:\\n‚ö†Ô∏è  WARNING SIGNS TO WATCH:\\n\\nProvide quick, actionable advice to save the plant!'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/11 18:41:30 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: plant_care_emergency, version 15\n"
     ]
    }
   ],
   "source": [
    "uri_list = []\n",
    "for prompt in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    PROMPT_V2 = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt[\"template\"].split(\"{{question}}\")[0] + \n",
    "                \"Your response must be plain text only, without any formatting, bullet points, icons, or emojis, quotation marks, single quotation mark and accents. For example: I'm shall be I m\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                # Use double curly braces to indicate variables.\n",
    "                \"content\": \"Question: {{question}}\" + prompt[\"template\"].split(\"{{question}}\")[1],\n",
    "            },\n",
    "        ]\n",
    "    print(PROMPT_V2)\n",
    "    uri = mlflow.genai.register_prompt(\n",
    "        name=prompt[\"name\"],\n",
    "        template=PROMPT_V2,\n",
    "        commit_message=\"Update prompt Format\",\n",
    "        tags=prompt[\"tags\"],\n",
    "    )\n",
    "    uri_list.append(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "786429c2-da24-45a0-95d5-09d907ebe7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptVersion(name=plant_care_basic, version=17, template=[{\"role\": \"system\", \"content\":...),\n",
       " PromptVersion(name=plant_care_structured, version=15, template=[{\"role\": \"system\", \"content\":...),\n",
       " PromptVersion(name=plant_care_diagnostic, version=15, template=[{\"role\": \"system\", \"content\":...),\n",
       " PromptVersion(name=plant_care_emergency, version=15, template=[{\"role\": \"system\", \"content\":...)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "283726d8-32a9-44d6-b49e-b4e6bfba0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.openai.autolog()\n",
    "client = AzureOpenAI(\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "@mlflow.trace\n",
    "def predict_fn(question: str) -> str:\n",
    "    prompt = mlflow.genai.load_prompt(f\"prompts:/plant_care_structured/15\")\n",
    "    #prompt = mlflow.genai.load_prompt(uri_list[0])\n",
    "    rendered_prompt = prompt.format(question=question)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=os.getenv(\"AZURE_DEPLOYMENT_NAME\"), messages=rendered_prompt\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b2509f77-7691-4eae-acde-07b387a20058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities import AssessmentSource, Feedback\n",
    "from mlflow.genai import scorer\n",
    "\n",
    "def check_concepts_flexibly(concepts, outputs):\n",
    "    outputs_lower = outputs.lower()\n",
    "    included_concepts = set()\n",
    "    for concept in concepts:\n",
    "        # Divide o conceito em palavras-chave (ex: \"bonsai only\" -> [\"bonsai\", \"only\"])\n",
    "        concept_words = concept.lower().split()\n",
    "        # Verifica se todas as palavras-chave do conceito est√£o na sa√≠da\n",
    "        if all(word in outputs_lower for word in concept_words):\n",
    "            included_concepts.add(concept)\n",
    "    return included_concepts\n",
    "\n",
    "# Evaluate the coverage of the key concepts using custom scorer\n",
    "@scorer\n",
    "def concept_coverage(outputs: str, expectations: dict) -> Feedback:\n",
    "    concepts = set(expectations.get(\"key_concepts\", []))\n",
    "    included = check_concepts_flexibly(concepts, outputs)\n",
    "    return Feedback(\n",
    "        name=\"concept_coverage\",\n",
    "        value=(len(included) / len(concepts)),\n",
    "        rationale=f\"Included {len(included)} out of {len(concepts)} concepts. Missing: {concepts - included}\",\n",
    "        source=AssessmentSource(\n",
    "            source_type=\"HUMAN\",\n",
    "            source_id=\"john@example.com\",\n",
    "        ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ff80a693-13bd-4712-99fd-310259a380a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer\n",
    "def llm_judged_correctness(outputs: str, expectations: Dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    A custom scorer that uses an LLM to judge the correctness of an output\n",
    "    against a ground truth expectation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The ground truth is expected to be in the 'expectations' dictionary\n",
    "    ground_truth = expectations.get(\"expected_response\")\n",
    "    if not ground_truth:\n",
    "        return Feedback(\n",
    "            name=\"llm_judged_correctness\",\n",
    "            value=0, # Score 0 if no ground truth is provided\n",
    "            rationale=\"Failed: The 'expectations' dictionary did not contain an 'expected_response' key.\",\n",
    "        )\n",
    "\n",
    "    # This is the prompt that instructs our judge LLM. It is the most critical part.\n",
    "    grading_prompt = f\"\"\"\n",
    "    You are an impartial AI judge. Your task is to evaluate the correctness of a generated answer based on a ground truth answer.\n",
    "\n",
    "    SCORING CRITERIA:\n",
    "    Score on a scale of 1 to 5, where 5 is best.\n",
    "    1: The answer is completely incorrect or irrelevant.\n",
    "    3: The answer is partially correct but misses key details or contains inaccuracies.\n",
    "    5: The answer is fully correct, complete, and aligns perfectly with the ground truth.\n",
    "\n",
    "    YOUR TASK:\n",
    "    Evaluate the following generated answer against the ground truth.\n",
    "\n",
    "    GROUND TRUTH:\n",
    "    \"{ground_truth}\"\n",
    "\n",
    "    GENERATED ANSWER:\n",
    "    \"{outputs}\"\n",
    "\n",
    "    OUTPUT FORMAT (CRITICAL):\n",
    "    You MUST respond with a single, valid JSON object and nothing else. The JSON object must contain two keys: \"score\" (an integer from 1 to 5) and \"justification\" (a brief, one-sentence explanation for your score).\n",
    "    Ensure all special characters within the justification string are correctly escaped.\n",
    "    \n",
    "    EXAMPLE:\n",
    "    {{\"score\": 4, \"justification\": \"The answer is correct but could be more concise.\"}}\n",
    "    \"\"\"\n",
    "    JUDGE_MODEL_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "    try:\n",
    "        # Call the judge LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=JUDGE_MODEL_DEPLOYMENT_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": grading_prompt}],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"}, # Force JSON output\n",
    "        )\n",
    "        \n",
    "        # Parse the JSON response from the judge\n",
    "        judge_response_text = response.choices[0].message.content\n",
    "        parsed_response = json.loads(judge_response_text)\n",
    "        \n",
    "        score = parsed_response.get(\"score\")\n",
    "        justification = parsed_response.get(\"justification\")\n",
    "        \n",
    "        if score is None or justification is None:\n",
    "            raise ValueError(\"Judge model response did not contain 'score' or 'justification'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # If the judge fails (e.g., API error, malformed JSON), return a low score with an error message\n",
    "        return Feedback(\n",
    "            name=\"llm_judged_correctness\",\n",
    "            value=0,\n",
    "            rationale=f\"Failed to get a valid score from the judge model. Error: {str(e)}\",\n",
    "        )\n",
    "\n",
    "    # Return the final Feedback object\n",
    "    return Feedback(\n",
    "        name=\"llm_judged_correctness\",\n",
    "        value=score, # The score from the judge LLM\n",
    "        rationale=justification, # The justification from the judge LLM\n",
    "        source=AssessmentSource(\n",
    "            source_type=\"LLM_JUDGE\",\n",
    "            source_id=f\"azure_openai:/{JUDGE_MODEL_DEPLOYMENT_NAME}\",\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d99e88a1-31a3-4e67-93bc-1581363e1f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Bonsai-Care-Prompt-Engineering' already exists. Creating a new version of this model...\n",
      "2025/09/11 19:28:40 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Bonsai-Care-Prompt-Engineering, version 13\n",
      "Created version '13' of model 'Bonsai-Care-Prompt-Engineering'.\n",
      "2025/09/11 19:28:40 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [Elapsed: 00:22, Remaining: 00:00] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://mlflow:5000/#/experiments/1/runs/d326cf8e7b474a9897f2965020e7eac4/traces\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlflow.genai.scorers import Correctness\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Use the optimized prompt in your application\n",
    "    model_info = mlflow.openai.log_model(\n",
    "        model=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "        task=\"chat.completions\",\n",
    "        name=EXPERIMENT_NAME,\n",
    "        registered_model_name=EXPERIMENT_NAME,\n",
    "        prompts=[uri_list[0]],  # Link optimized prompt to model\n",
    "        messages = uri.template\n",
    "    )\n",
    "    scorers = [\n",
    "        #Correctness(model=\"azure:/gpt-4o\"),\n",
    "        llm_judged_correctness,\n",
    "        concept_coverage\n",
    "    ]\n",
    "    \n",
    "    results = mlflow.genai.evaluate(\n",
    "        data=eval_dataset,\n",
    "        predict_fn=predict_fn,\n",
    "        scorers=scorers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "840db2f6-8a20-47c1-b69a-4d9bcd6add2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_function': {'loader_module': 'mlflow.openai',\n",
       "  'python_version': '3.11.6',\n",
       "  'data': 'model.yaml',\n",
       "  'env': {'conda': 'conda.yaml', 'virtualenv': 'python_env.yaml'}},\n",
       " 'openai': {'openai_version': '1.107.0', 'data': 'model.yaml', 'code': None}}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.flavors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "93941e57-4084-45dc-8463-5fa4d379aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model 'Bonsai-Care-Prompt-Engineering' version 13 has been logged and registered.\n",
      "\n",
      "--- Step 2: Transitioning version 13 to 'Staging' ---\n",
      "‚úÖ Version 13 successfully transitioned to 'Staging'.\n",
      "\n",
      "   Loading model from Staging for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105/3553927183.py:13: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n",
      "2025/09/11 19:29:36 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-1ac8918b4bfd488c8839f4c2cb421b47\n",
      "2025/09/11 19:29:36 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Test response from Staging model: Nebari is a term used in bonsai cultivation to describe the visible surface roots of a tree that spread out from the base of the trunk. The nebari is an essential feature in bonsai design because it adds to the tree's visual stability, age, and natural appearance. If you're asking because of concerns with your bonsai, ensure its potting medium allows proper root exposure and health.\n"
     ]
    }
   ],
   "source": [
    "# Deploy the best-performing version\n",
    "model_version = model_info.registered_model_version\n",
    "print(f\"‚úÖ Model '{EXPERIMENT_NAME}' version {model_version} has been logged and registered.\")\n",
    "\n",
    "\n",
    "# --- 3. Transition the Model Version to Staging ---\n",
    "# This step would typically happen after some automated validation or manual review.\n",
    "print(f\"\\n--- Step 2: Transitioning version {model_version} to 'Staging' ---\")\n",
    "\n",
    "# Initialize the MLflow client to interact with the Model Registry\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=EXPERIMENT_NAME,\n",
    "    version=model_version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=True # This will move any existing 'Staging' model to 'Archived'\n",
    ")\n",
    "print(f\"‚úÖ Version {model_version} successfully transitioned to 'Staging'.\")\n",
    "\n",
    "# You can now load the 'Staging' model in your testing environment like this:\n",
    "print(\"\\n   Loading model from Staging for testing...\")\n",
    "try:\n",
    "    staging_model = mlflow.pyfunc.load_model(f\"models:/{EXPERIMENT_NAME}/Staging\")\n",
    "    response = staging_model.predict([{\"question\": \"What is nebari?\"}])\n",
    "    print(f\"   Test response from Staging model: {response[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Failed to load staging model. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b71b374c-7c44-4715-a526-c120ce82c74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Promoting version 13 to 'Production' ---\n",
      "üéâ Version 13 successfully promoted to 'Production'!\n",
      "\n",
      "   Loading model from Production for application use...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105/1986521641.py:5: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Response from Production model: IMMEDIATE RESPONSE PROTOCOL:  \n",
      "URGENT ACTIONS (Next 24 hours): Water your Ficus bonsai thoroughly. Pour water over the soil until it begins to drain out of the bottom holes. Make sure the pot has good drainage to avoid waterlogging. Use room-temperature water.  \n",
      "ASSESSMENT NEEDED: Check the soil moisture by touching it with your finger about an inch deep. Water only if the soil feels slightly dry. Avoid letting it completely dry out or stay overly soggy.  \n",
      "MONITORING PLAN: Monitor the soil's moisture daily. Ensure a consistent watering routine and adjust depending on environmental factors like temperature and humidity.  \n",
      "WARNING SIGNS TO WATCH: Drooping leaves, yellowing, or leaves falling off may indicate overwatering, while dry, crispy leaves signal underwatering. Avoid these extremes.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Promote the Model Version to Production ---\n",
    "# This is the final step after the model has passed all staging tests.\n",
    "print(f\"\\n--- Step 3: Promoting version {model_version} to 'Production' ---\")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=EXPERIMENT_NAME,\n",
    "    version=model_version,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True # Move the old 'Production' model to 'Archived'\n",
    ")\n",
    "print(f\"üéâ Version {model_version} successfully promoted to 'Production'!\")\n",
    "\n",
    "# Your production application can now reliably load the latest approved model.\n",
    "print(\"\\n   Loading model from Production for application use...\")\n",
    "try:\n",
    "    prod_model = mlflow.pyfunc.load_model(f\"models:/{EXPERIMENT_NAME}/Production\")\n",
    "    response = prod_model.predict([{\"question\": \"How do I water a Ficus bonsai?\"}])\n",
    "    print(f\"   Response from Production model: {response[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Failed to load production model. Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a784ec-ddf9-4fdc-8a7f-de9b6030c222",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "In this session, we have explored three key components of LLMops using MLflow:\n",
    "\n",
    "1.  **Prompt Registry**: We learned how to register, version, and load prompts as reproducible artifacts.\n",
    "2.  **Tracing**: We saw how to trace LLM calls to gain observability into our application's behavior.\n",
    "3.  **Evaluation**: We used `mlflow.genai.evaluate` to systematically compare the performance of different prompts.\n",
    "\n",
    "These tools provide a powerful framework for developing, monitoring, and improving production-ready GenAI applications.\n",
    "\n",
    "**Next Steps:**\n",
    "- Explore the MLflow UI to see the logged runs, traces, and evaluation results.\n",
    "- Experiment with other prompt engineering techniques (e.g., few-shot prompting).\n",
    "- Create custom evaluation metrics tailored to your specific use case.\n",
    "- Integrate this workflow into a CI/CD pipeline for continuous improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632d62e-a3a9-4dea-ac84-bf41de0fa11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
